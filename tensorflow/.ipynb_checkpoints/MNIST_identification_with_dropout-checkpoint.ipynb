{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-7a8c537ebeb9>:139: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "loss of iteration 0 : 1.829579\n",
      "accuracy is: 0.724600\n",
      "loss of iteration 1 : 1.735072\n",
      "accuracy is: 0.797400\n",
      "loss of iteration 2 : 1.709098\n",
      "accuracy is: 0.815200\n",
      "loss of iteration 3 : 1.664822\n",
      "accuracy is: 0.851000\n",
      "loss of iteration 4 : 1.627203\n",
      "accuracy is: 0.882700\n",
      "loss of iteration 5 : 1.613127\n",
      "accuracy is: 0.891400\n",
      "loss of iteration 6 : 1.601581\n",
      "accuracy is: 0.897900\n",
      "loss of iteration 7 : 1.593241\n",
      "accuracy is: 0.903200\n",
      "loss of iteration 8 : 1.587226\n",
      "accuracy is: 0.907900\n",
      "loss of iteration 9 : 1.582638\n",
      "accuracy is: 0.910800\n",
      "loss of iteration 10 : 1.578928\n",
      "accuracy is: 0.913600\n",
      "loss of iteration 11 : 1.575781\n",
      "accuracy is: 0.915500\n",
      "loss of iteration 12 : 1.573021\n",
      "accuracy is: 0.917000\n",
      "loss of iteration 13 : 1.570552\n",
      "accuracy is: 0.918700\n",
      "loss of iteration 14 : 1.568310\n",
      "accuracy is: 0.920200\n",
      "loss of iteration 15 : 1.566247\n",
      "accuracy is: 0.922600\n",
      "loss of iteration 16 : 1.564326\n",
      "accuracy is: 0.924600\n",
      "loss of iteration 17 : 1.562523\n",
      "accuracy is: 0.925800\n",
      "loss of iteration 18 : 1.560824\n",
      "accuracy is: 0.926700\n",
      "loss of iteration 19 : 1.559222\n",
      "accuracy is: 0.928200\n",
      "loss of iteration 20 : 1.557707\n",
      "accuracy is: 0.928900\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import struct\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "dataset_dir = 'datasets/mnist_database/'\n",
    "train_set_path = os.path.join(dataset_dir, 'train-images.idx3-ubyte')\n",
    "train_label_path = os.path.join(dataset_dir, 'train-labels.idx1-ubyte')\n",
    "test_set_path = os.path.join(dataset_dir, 't10k-images.idx3-ubyte')\n",
    "test_label_path = os.path.join(dataset_dir, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "def load_data(file_path):\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"The file is not exist!\")\n",
    "        return\n",
    "    \n",
    "    fr_binary = open(file_path, 'rb')\n",
    "    buffer = fr_binary.read()\n",
    "\n",
    "    if re.search('\\w+-(images)\\.', os.path.split(file_path)[-1]) is not None:\n",
    "        \"\"\"\n",
    "        提取数据文件\n",
    "        \"\"\"\n",
    "        head = struct.unpack_from('>IIII', buffer, 0)\n",
    "        offset = struct.calcsize('>IIII')       # 定位到字节流中 data 开始的位置\n",
    "        img_num, width, height = head[1:]   \n",
    "        format_str = '>{0}B'.format(img_num * width * height)\n",
    "        data = struct.unpack_from(format_str, buffer, offset)\n",
    "        fr_binary.close()\n",
    "        data = np.reshape(data, [img_num, width, height])\n",
    "        return data\n",
    "    \n",
    "    elif re.search('\\w+-(labels)\\.', file_path) is not None:\n",
    "        \"\"\"\n",
    "        提取标签文件\n",
    "        \"\"\"\n",
    "        head = struct.unpack_from('>II', buffer, 0)\n",
    "        label_num = head[1]\n",
    "        offset = struct.calcsize('>II')\n",
    "        format_str = '>{0}B'.format(label_num)\n",
    "        labels = struct.unpack_from(format_str, buffer, offset)\n",
    "        fr_binary.close()\n",
    "        labels = np.reshape(labels, [label_num])\n",
    "        return labels\n",
    "        \n",
    "train_img = load_data(train_set_path)\n",
    "train_label = load_data(train_label_path)\n",
    "test_img = load_data(test_set_path)\n",
    "test_label = load_data(test_label_path)\n",
    "\n",
    "# 调整数据结构\n",
    "def convert_one_hot(y, num_classes):\n",
    "    \n",
    "    y_one_hot = np.zeros((y.shape[-1], num_classes))\n",
    "    for i in range(y.shape[-1]):\n",
    "        y_one_hot[i][y[i]] = 1\n",
    "    return y_one_hot\n",
    "    \n",
    "train_label_one_hot = convert_one_hot(train_label, 10)\n",
    "test_label_one_hot = convert_one_hot(test_label, 10)\n",
    "train_data = (np.reshape(train_img, (60000, 784)) / 255).astype(np.float32)\n",
    "test_data = (np.reshape(test_img, (10000, 784)) / 255).astype(np.float32)\n",
    "\n",
    "# 生成batches\n",
    "def get_batches(X, y, batch_size, axis = 0, seed = 0):\n",
    "    \n",
    "    assert(X.shape[axis] == y.shape[axis])\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[axis]\n",
    "    mini_batches = []\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    num_complete_minibatches = m // batch_size\n",
    "    \n",
    "    if 0 == axis:\n",
    "        shuffled_X = X[permutation, :]\n",
    "        shuffled_y = y[permutation, :]\n",
    "        for k in range(num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[k * batch_size: (k + 1) * batch_size, :]\n",
    "            mini_batch_y = shuffled_y[k * batch_size: (k + 1) * batch_size, :]\n",
    "            mini_batches.append((mini_batch_X, mini_batch_y))\n",
    "        if m % batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[num_complete_minibatches * batch_size, :]\n",
    "            mini_batch_y = shuffled_y[num_complete_minibatches * batch_size, :]\n",
    "            mini_batches.append((mini_batch_X, mini_batch_y))\n",
    "        return mini_batches\n",
    "        \n",
    "    elif 1 == axis:\n",
    "        shuffled_X = X[:, permutation]\n",
    "        shuffled_y = y[:, permutation]\n",
    "        for k in range(num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[:, k * batch_size: (k + 1) * batch_size]\n",
    "            mini_batch_y = shuffled_y[:, k * batch_size: (k + 1) * batch_size]\n",
    "            mini_batches.append((mini_batch_X, mini_batch_y))\n",
    "        if m % batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[:, num_complete_minibatches * batch_size]\n",
    "            mini_batch_y = shuffled_y[:, num_complete_minibatches * batch_size]\n",
    "            mini_batches.append((mini_batch_X, mini_batch_y))\n",
    "        return mini_batches\n",
    "    \n",
    "# 搭建一个简单的神经网络（搭建计算图）\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)       # 用于存储保留节点的比例\n",
    "\n",
    "# 设置网络第一层的权重及偏置\n",
    "W1 = tf.Variable(tf.truncated_normal([784, 500], stddev = 0.1))   # 节点数为2000\n",
    "b1 = tf.Variable(tf.zeros([1, 500]))\n",
    "Z1 = tf.matmul(X, W1) + b1\n",
    "L1 = tf.nn.tanh(Z1)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "# 设置第二层网络\n",
    "W2 = tf.Variable(tf.truncated_normal([500, 300], stddev = 0.1))   # 节点数为2000\n",
    "b2 = tf.Variable(tf.zeros([1, 300]))\n",
    "Z2 = tf.matmul(L1_drop, W2) + b2\n",
    "L2 = tf.nn.tanh(Z2)\n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "# 设置第三层网络\n",
    "W3 = tf.Variable(tf.truncated_normal([300, 10], stddev = 0.1))\n",
    "b3 = tf.Variable(tf.zeros([1, 10]))\n",
    "Z3 = tf.matmul(L2_drop, W3) + b3\n",
    "prediction = tf.nn.softmax(Z3)\n",
    "# L3 = tf.nn.tanh(Z3)\n",
    "# L3_drop = tf.nn.dropout(L3, keep_prob)\n",
    "\n",
    "# # 设置输出层\n",
    "# W4 = tf.Variable(tf.truncated_normal([100, 10], stddev = 0.1))\n",
    "# b4 = tf.Variable(tf.zeros([1, 10]))\n",
    "# Z4 = tf.matmul(L3_drop, W4) + b4\n",
    "# prediction = tf.nn.softmax(Z4)\n",
    "\n",
    "# 使用L2代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "# 使用cross entropy代价函数\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits = prediction))\n",
    "\n",
    "# 使用梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = train_step.minimize(loss)\n",
    "\n",
    "# 变量初始化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 计算准确率，结果存放在一个 boolean 型列表中\n",
    "correction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))   # argmax返回最大值所在的位置\n",
    "accuracy = tf.reduce_mean(tf.cast(correction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:       # 创建 session 训练网络\n",
    "    sess.run(init)   # 先执行变量初始化\n",
    "    mini_batches = get_batches(train_data, train_label_one_hot, batch_size = 100)\n",
    "    for i in range(21):\n",
    "        for (X_data, y_data) in mini_batches:\n",
    "            sess.run(train, feed_dict = {X: X_data, y: y_data, keep_prob: 1.0})\n",
    "            loss_value = sess.run(loss, feed_dict = {X: X_data, y: y_data, keep_prob: 1.0}) \n",
    "        if 0 == i % 1:\n",
    "            acc = sess.run(accuracy, feed_dict = {X: test_data, y: test_label_one_hot, keep_prob: 1.0})   \n",
    "            print(\"loss of iteration %d : %f\" %(i, loss_value))\n",
    "            print(\"accuracy is: %f\" %(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
