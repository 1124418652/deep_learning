{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import struct\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "dataset_dir = 'datasets/mnist_database/'\n",
    "train_set_path = os.path.join(dataset_dir, 'train-images.idx3-ubyte')\n",
    "train_label_path = os.path.join(dataset_dir, 'train-labels.idx1-ubyte')\n",
    "test_set_path = os.path.join(dataset_dir, 't10k-images.idx3-ubyte')\n",
    "test_label_path = os.path.join(dataset_dir, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "def load_data(file_path):\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"The file is not exist!\")\n",
    "        return\n",
    "    \n",
    "    fr_binary = open(file_path, 'rb')\n",
    "    buffer = fr_binary.read()\n",
    "\n",
    "    if re.search('\\w+-(images)\\.', os.path.split(file_path)[-1]) is not None:\n",
    "        \"\"\"\n",
    "        提取数据文件\n",
    "        \"\"\"\n",
    "        head = struct.unpack_from('>IIII', buffer, 0)\n",
    "        offset = struct.calcsize('>IIII')       # 定位到字节流中 data 开始的位置\n",
    "        img_num, width, height = head[1:]   \n",
    "        format_str = '>{0}B'.format(img_num * width * height)\n",
    "        data = struct.unpack_from(format_str, buffer, offset)\n",
    "        fr_binary.close()\n",
    "        data = np.reshape(data, [img_num, width, height])\n",
    "        return data\n",
    "    \n",
    "    elif re.search('\\w+-(labels)\\.', file_path) is not None:\n",
    "        \"\"\"\n",
    "        提取标签文件\n",
    "        \"\"\"\n",
    "        head = struct.unpack_from('>II', buffer, 0)\n",
    "        label_num = head[1]\n",
    "        offset = struct.calcsize('>II')\n",
    "        format_str = '>{0}B'.format(label_num)\n",
    "        labels = struct.unpack_from(format_str, buffer, offset)\n",
    "        fr_binary.close()\n",
    "        labels = np.reshape(labels, [label_num])\n",
    "        return labels\n",
    "        \n",
    "train_img = load_data(train_set_path)\n",
    "train_label = load_data(train_label_path)\n",
    "test_img = load_data(test_set_path)\n",
    "test_label = load_data(test_label_path)\n",
    "\n",
    "# 调整数据结构\n",
    "def convert_one_hot(y, num_classes):\n",
    "    \n",
    "    y_one_hot = np.zeros((y.shape[-1], num_classes))\n",
    "    for i in range(y.shape[-1]):\n",
    "        y_one_hot[i][y[i]] = 1\n",
    "    return y_one_hot\n",
    "    \n",
    "train_label_one_hot = convert_one_hot(train_label, 10)\n",
    "test_label_one_hot = convert_one_hot(test_label, 10)\n",
    "train_data = (np.reshape(train_img, (60000, 784)) / 255).astype(np.float32)\n",
    "test_data = (np.reshape(test_img, (10000, 784)) / 255).astype(np.float32)\n",
    "\n",
    "# 生成batches\n",
    "def get_batches(X, y, batch_size, axis = 0, seed = 0):\n",
    "    \n",
    "    assert(X.shape[axis] == y.shape[axis])\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[axis]\n",
    "    mini_batches = []\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    num_complete_minibatches = m // batch_size\n",
    "    \n",
    "    if 0 == axis:\n",
    "        shuffled_X = X[permutation, :]\n",
    "        shuffled_y = y[permutation, :]\n",
    "        for k in range(num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[k * batch_size: (k + 1) * batch_size, :]\n",
    "            mini_batch_y = shuffled_y[k * batch_size: (k + 1) * batch_size, :]\n",
    "            mini_batches.append((mini_batch_X, mini_batch_y))\n",
    "        if m % batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[num_complete_minibatches * batch_size, :]\n",
    "            mini_batch_y = shuffled_y[num_complete_minibatches * batch_size, :]\n",
    "            mini_batches.append((mini_batch_X, mini_batch_y))\n",
    "        return mini_batches\n",
    "        \n",
    "    elif 1 == axis:\n",
    "        shuffled_X = X[:, permutation]\n",
    "        shuffled_y = y[:, permutation]\n",
    "        for k in range(num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[:, k * batch_size: (k + 1) * batch_size]\n",
    "            mini_batch_y = shuffled_y[:, k * batch_size: (k + 1) * batch_size]\n",
    "            mini_batches.append((mini_batch_X, mini_batch_y))\n",
    "        if m % batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[:, num_complete_minibatches * batch_size]\n",
    "            mini_batch_y = shuffled_y[:, num_complete_minibatches * batch_size]\n",
    "            mini_batches.append((mini_batch_X, mini_batch_y))\n",
    "        return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-57aaf21a5355>:89: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 构建卷积网络\n",
    "\n",
    "def weight_variable(shape):\n",
    "    # 初始化权重矩阵\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)     # 生成一个截断的正态分布\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    # 初始化偏置矩阵\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, strides = [1, 1, 1, 1], padding = 'SAME'):\n",
    "    \"\"\"\n",
    "    Calculate the convolve of x and W\n",
    "    \n",
    "    Arguments:\n",
    "    x -- input tensor of shape (batch_num, height, width, channels)\n",
    "    W -- tensorflow Variable, with the shape of \n",
    "         (filter_height, filter_width, in_channels, out_channels)\n",
    "    stride -- python list, respect to the stride of every dimensions\n",
    "    padding -- the type of padding, {'SAME', 'VALID'}\n",
    "         \n",
    "    Returns:\n",
    "    tensor after convolve\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.nn.conv2d(x, W, strides = strides, padding = padding)\n",
    "\n",
    "def pool(x, pool_type = 'max', ksize = [1, 2, 2, 1], \n",
    "         strides = [1, 2, 2, 1], padding = 'SAME'):\n",
    "    \"\"\"\n",
    "    the pool layer of convolution network\n",
    "    \n",
    "    Arguments:\n",
    "    x -- input tensor of shape (batch_num, height, width, channels)\n",
    "    mode -- the type of pool layer {'max', 'average'}\n",
    "    ksize -- A list or tuple of 4 ints. The size of the window for each dimension\n",
    "             of the input tensor.\n",
    "    strides -- A list or tuple of 4 ints. The strides of the sliding window for\n",
    "               each dimension of the input tensor.\n",
    "    padding -- the type of padding, {'SAME', 'VALID'}\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'max' == pool_type:\n",
    "        return tf.nn.max_pool(x, ksize = ksize, strides = strides, padding = padding)\n",
    "    elif 'average' == pool_type:\n",
    "        return tf.nn.avg_pool(x, ksize = ksize, strides = strides, padding = padding)\n",
    "    \n",
    "# 定义网络的输入\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])    # reshape the input x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "定义网络的结构: \n",
    "conv + relu + max_pool + conv + relu + max_pool + full_connection(relu) \n",
    "+ softmax\n",
    "\"\"\"\n",
    "# layer1,input dimension (-1, 28, 28, 1), output dimension (-1, 14, 14, 32)\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "a_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "a_conv1_pool = pool(a_conv1)\n",
    "\n",
    "# layer2, input dimension (-1, 14, 14, 32), output dimension (-1, 7, 7, 64)\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "a_conv2 = tf.nn.relu(conv2d(a_conv1_pool, W_conv2) + b_conv2)\n",
    "a_conv2_pool = pool(a_conv2)\n",
    "\n",
    "# full connection layer1, reshape the dimension of input first\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "A_input = tf.reshape(a_conv2_pool, [-1, 7 * 7 * 64])\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])     # 第一层全连接层有1024个神经元\n",
    "b_fc1 = bias_variable([1024])\n",
    "Z_fc1 = tf.matmul(A_input, W_fc1) + b_fc1\n",
    "A_fc1 = tf.nn.relu(Z_fc1)\n",
    "A_fc1_prob = tf.nn.dropout(A_fc1, keep_prob)\n",
    "\n",
    "# full connection layer2, softmax layer\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "Z_fc2 = tf.matmul(A_fc1_prob, W_fc2) + b_fc2\n",
    "prediction = tf.nn.softmax(Z_fc2)\n",
    "\n",
    "# calculate cross entropy\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = prediction))\n",
    "# 使用 Adam 进行网络参数的优化\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "# 计算准确率\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, axis = 1), tf.argmax(y, axis = 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 , Testing Accuracy = 0.956\n",
      "Iter 1 , Testing Accuracy = 0.9727\n",
      "Iter 2 , Testing Accuracy = 0.9795\n",
      "Iter 3 , Testing Accuracy = 0.9819\n",
      "Iter 4 , Testing Accuracy = 0.9839\n",
      "Iter 5 , Testing Accuracy = 0.986\n",
      "Iter 6 , Testing Accuracy = 0.9862\n",
      "Iter 7 , Testing Accuracy = 0.9873\n",
      "Iter 8 , Testing Accuracy = 0.9876\n",
      "Iter 9 , Testing Accuracy = 0.9889\n",
      "Iter 10 , Testing Accuracy = 0.9898\n",
      "Iter 11 , Testing Accuracy = 0.9895\n",
      "Iter 12 , Testing Accuracy = 0.9889\n",
      "Iter 13 , Testing Accuracy = 0.9904\n",
      "Iter 14 , Testing Accuracy = 0.9898\n",
      "Iter 15 , Testing Accuracy = 0.9915\n",
      "Iter 16 , Testing Accuracy = 0.9916\n",
      "Iter 17 , Testing Accuracy = 0.9916\n",
      "Iter 18 , Testing Accuracy = 0.9915\n",
      "Iter 19 , Testing Accuracy = 0.9917\n",
      "Iter 20 , Testing Accuracy = 0.9923\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())       # 初始化网络中的所有变量\n",
    "    mini_batches = get_batches(train_data, train_label_one_hot, batch_size=100)\n",
    "    for epoch in range(21):\n",
    "        for X_data, y_data in mini_batches:\n",
    "            sess.run(train_step, feed_dict = {x: X_data, y: y_data, keep_prob: 0.7})\n",
    "        acc = sess.run(accuracy, feed_dict = {x: test_data, y: test_label_one_hot, keep_prob: 1})\n",
    "        print('Iter ' + str(epoch) + ' , Testing Accuracy = ' + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
