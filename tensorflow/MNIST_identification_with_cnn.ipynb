{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import struct\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "dataset_dir = 'datasets/mnist_database/'\n",
    "train_set_path = os.path.join(dataset_dir, 'train-images.idx3-ubyte')\n",
    "train_label_path = os.path.join(dataset_dir, 'train-labels.idx1-ubyte')\n",
    "test_set_path = os.path.join(dataset_dir, 't10k-images.idx3-ubyte')\n",
    "test_label_path = os.path.join(dataset_dir, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "def load_data(file_path):\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"The file is not exist!\")\n",
    "        return\n",
    "    \n",
    "    fr_binary = open(file_path, 'rb')\n",
    "    buffer = fr_binary.read()\n",
    "\n",
    "    if re.search('\\w+-(images)\\.', os.path.split(file_path)[-1]) is not None:\n",
    "        \"\"\"\n",
    "        提取数据文件\n",
    "        \"\"\"\n",
    "        head = struct.unpack_from('>IIII', buffer, 0)\n",
    "        offset = struct.calcsize('>IIII')       # 定位到字节流中 data 开始的位置\n",
    "        img_num, width, height = head[1:]   \n",
    "        format_str = '>{0}B'.format(img_num * width * height)\n",
    "        data = struct.unpack_from(format_str, buffer, offset)\n",
    "        fr_binary.close()\n",
    "        data = np.reshape(data, [img_num, width, height])\n",
    "        return data\n",
    "    \n",
    "    elif re.search('\\w+-(labels)\\.', file_path) is not None:\n",
    "        \"\"\"\n",
    "        提取标签文件\n",
    "        \"\"\"\n",
    "        head = struct.unpack_from('>II', buffer, 0)\n",
    "        label_num = head[1]\n",
    "        offset = struct.calcsize('>II')\n",
    "        format_str = '>{0}B'.format(label_num)\n",
    "        labels = struct.unpack_from(format_str, buffer, offset)\n",
    "        fr_binary.close()\n",
    "        labels = np.reshape(labels, [label_num])\n",
    "        return labels\n",
    "        \n",
    "train_img = load_data(train_set_path)\n",
    "train_label = load_data(train_label_path)\n",
    "test_img = load_data(test_set_path)\n",
    "test_label = load_data(test_label_path)\n",
    "\n",
    "# 调整数据结构\n",
    "def convert_one_hot(y, num_classes):\n",
    "    \n",
    "    y_one_hot = np.zeros((y.shape[-1], num_classes))\n",
    "    for i in range(y.shape[-1]):\n",
    "        y_one_hot[i][y[i]] = 1\n",
    "    return y_one_hot\n",
    "    \n",
    "train_label_one_hot = convert_one_hot(train_label, 10)\n",
    "test_label_one_hot = convert_one_hot(test_label, 10)\n",
    "train_data = (np.reshape(train_img, (60000, 784)) / 255).astype(np.float32)\n",
    "test_data = (np.reshape(test_img, (10000, 784)) / 255).astype(np.float32)\n",
    "\n",
    "# 生成batches\n",
    "def get_batches(X, y, batch_size, axis = 0, seed = 0):\n",
    "    \n",
    "    assert(X.shape[axis] == y.shape[axis])\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[axis]\n",
    "    mini_batches = []\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    num_complete_minibatches = m // batch_size\n",
    "    \n",
    "    if 0 == axis:\n",
    "        shuffled_X = X[permutation, :]\n",
    "        shuffled_y = y[permutation, :]\n",
    "        for k in range(num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[k * batch_size: (k + 1) * batch_size, :]\n",
    "            mini_batch_y = shuffled_y[k * batch_size: (k + 1) * batch_size, :]\n",
    "            mini_batches.append((mini_batch_X, mini_batch_y))\n",
    "        if m % batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[num_complete_minibatches * batch_size, :]\n",
    "            mini_batch_y = shuffled_y[num_complete_minibatches * batch_size, :]\n",
    "            mini_batches.append((mini_batch_X, mini_batch_y))\n",
    "        return mini_batches\n",
    "        \n",
    "    elif 1 == axis:\n",
    "        shuffled_X = X[:, permutation]\n",
    "        shuffled_y = y[:, permutation]\n",
    "        for k in range(num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[:, k * batch_size: (k + 1) * batch_size]\n",
    "            mini_batch_y = shuffled_y[:, k * batch_size: (k + 1) * batch_size]\n",
    "            mini_batches.append((mini_batch_X, mini_batch_y))\n",
    "        if m % batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[:, num_complete_minibatches * batch_size]\n",
    "            mini_batch_y = shuffled_y[:, num_complete_minibatches * batch_size]\n",
    "            mini_batches.append((mini_batch_X, mini_batch_y))\n",
    "        return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建卷积网络\n",
    "\n",
    "def weight_variable(shape):\n",
    "    # 初始化权重矩阵\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)     # 生成一个截断的正态分布\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    # 初始化偏置矩阵\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, strides = [1, 1, 1, 1], padding = 'SAME'):\n",
    "    \"\"\"\n",
    "    Calculate the convolve of x and W\n",
    "    \n",
    "    Arguments:\n",
    "    x -- input tensor of shape (batch_num, height, width, channels)\n",
    "    W -- tensorflow Variable, with the shape of \n",
    "         (filter_height, filter_width, in_channels, out_channels)\n",
    "    stride -- python list, respect to the stride of every dimensions\n",
    "    padding -- the type of padding, {'SAME', 'VALID'}\n",
    "         \n",
    "    Returns:\n",
    "    tensor after convolve\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.nn.conv2d(x, W, strides = strides, padding = padding)\n",
    "\n",
    "def pool(x, pool_type = 'max', ksize = [1, 2, 2, 1], \n",
    "         strides = [1, 2, 2, 1], padding = 'SAME'):\n",
    "    \"\"\"\n",
    "    the pool layer of convolution network\n",
    "    \n",
    "    Arguments:\n",
    "    x -- input tensor of shape (batch_num, height, width, channels)\n",
    "    mode -- the type of pool layer {'max', 'average'}\n",
    "    ksize -- A list or tuple of 4 ints. The size of the window for each dimension\n",
    "             of the input tensor.\n",
    "    strides -- A list or tuple of 4 ints. The strides of the sliding window for\n",
    "               each dimension of the input tensor.\n",
    "    padding -- the type of padding, {'SAME', 'VALID'}\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'max' == pool_type:\n",
    "        return tf.nn.max_pool(x, ksize = ksize, strides = strides, padding = padding)\n",
    "    elif 'average' == pool_type:\n",
    "        return tf.nn.avg_pool(x, ksize = ksize, strides = strides, padding = padding)\n",
    "    \n",
    "# 定义网络的输入\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])    # reshape the input x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "定义网络的结构: \n",
    "conv + relu + max_pool + conv + relu + max_pool + full_connection(relu) \n",
    "+ softmax\n",
    "\"\"\"\n",
    "# layer1,input dimension (-1, 28, 28, 1), output dimension (-1, 14, 14, 32)\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "a_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "a_conv1_pool = pool(a_conv1)\n",
    "\n",
    "# layer2, input dimension (-1, 14, 14, 32), output dimension (-1, 7, 7, 64)\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "a_conv2 = tf.nn.relu(conv2d(a_conv1_pool, W_conv2) + b_conv2)\n",
    "a_conv2_pool = pool(a_conv2)\n",
    "\n",
    "# full connection layer1, reshape the dimension of input first\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "A_input = tf.reshape(a_conv2_pool, [-1, 7 * 7 * 64])\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])     # 第一层全连接层有1024个神经元\n",
    "b_fc1 = bias_variable([1024])\n",
    "Z_fc1 = tf.matmul(A_input, W_fc1) + b_fc1\n",
    "A_fc1 = tf.nn.relu(Z_fc1)\n",
    "A_fc1_prob = tf.nn.dropout(A_fc1, keep_prob)\n",
    "\n",
    "# full connection layer2, softmax layer\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "Z_fc2 = tf.matmul(A_fc1_prob, W_fc2) + b_fc2\n",
    "prediction = tf.nn.softmax(Z_fc2)\n",
    "\n",
    "# calculate cross entropy\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = prediction))\n",
    "# 使用 Adam 进行网络参数的优化\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "# 计算准确率\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, axis = 1), tf.argmax(y, axis = 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 , Testing Accuracy = 0.9587\n",
      "Iter 1 , Testing Accuracy = 0.9709\n",
      "Iter 2 , Testing Accuracy = 0.9784\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "GPU sync failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mD:\\software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: GPU sync failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-297bb4750066>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mX_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmini_batches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_label_one_hot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Iter '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' , Testing Accuracy = '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\Anoconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: GPU sync failed"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())       # 初始化网络中的所有变量\n",
    "    mini_batches = get_batches(train_data, train_label_one_hot, batch_size=100)\n",
    "    for epoch in range(21):\n",
    "        for X_data, y_data in mini_batches:\n",
    "            sess.run(train_step, feed_dict = {x: X_data, y: y_data, keep_prob: 0.7})\n",
    "        acc = sess.run(accuracy, feed_dict = {x: test_data, y: test_label_one_hot, keep_prob: 1})\n",
    "        print('Iter ' + str(epoch) + ' , Testing Accuracy = ' + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
