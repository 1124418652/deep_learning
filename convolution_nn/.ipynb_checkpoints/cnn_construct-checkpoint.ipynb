{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积网络中所需要的模块\n",
    "1. 卷积模块，包含以下函数：\n",
    " * 使用０扩充边界\n",
    " * 卷积窗口\n",
    " * 前向卷积\n",
    " * 反向卷积（可选）\n",
    "2. 池化模块，包含以下函数：\n",
    " * 前向池化\n",
    " * 创建掩码\n",
    " * 值分配\n",
    " * 反向池化（可选）\n",
    "\n",
    "## 边界填充 \n",
    "**边界填充的实现**  \n",
    "可以使用numpy中的pad函数来对矩阵进行边界填充  \n",
    "np.pad(arr, ((dim1_before, dim1_after), (dim2_before, dim2_after),...), 'constant')  \n",
    "其中：\n",
    "1. arr表示需要进行填充的矩阵\n",
    "2. ((dim1_before, dim1_after), (dim2_before, dim2_after),...)表示每一维前后填充的宽度\n",
    "3. 'constant'表示用常数进行填充，默认为 0 进行填充　　\n",
    "\n",
    "**边界填充的好处**  \n",
    "* 经过边界填充之后，可以使卷积层的输出保持大小不变。这对于建立较深层的神经网络十分重要。\n",
    "* 边界填充可以帮助我们保留更多的信息，在没有填充的情况下，卷积过程中图像边缘的极少数值会受到过滤器的影响从而导致信息丢失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  1  2  3  4  5  0  0]\n",
      "  [ 0  0  1  2 32  4  5  0  0]\n",
      "  [ 0  0  1  2  3  4 54  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0]]\n",
      "\n",
      " [[ 0  0  0  0  0  0  0  0  0]\n",
      "  [ 0  0  2  3  4  4  5  0  0]\n",
      "  [ 0  0  2  3  3  4  5  0  0]\n",
      "  [ 0  0  2  3  4  4  5  0  0]\n",
      "  [ 0  0  0  0  0  0  0  0  0]]]\n",
      "[[0 0 1 2 3 0 0]\n",
      " [0 0 2 3 4 0 0]\n",
      " [0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr3D = np.array([[[1,2,3,4,5],\n",
    "                   [1,2,32,4,5],\n",
    "                   [1,2,3,4,54]],\n",
    "                  [[2,3,4,4,5],\n",
    "                   [2,3,3,4,5],\n",
    "                   [2,3,4,4,5]]])\n",
    "print(np.pad(arr3D, ((0,0),(1,1),(2,2)), 'constant'))\n",
    "arr1 = np.array([[1,2,3],\n",
    "                 [2,3,4]])\n",
    "print(np.pad(arr1, ((0,1),(2,2)), 'constant'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f75f62d2400>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f75fb359a58>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# 定义0填充函数\n",
    "def zero_pad(x, pad):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    @ x: 图像数据集，维度为（样本数，图像高度，图像宽度，图像通道数）\n",
    "    @ pad: 整数，对图像数据集中每个样本的每一个通道（即R,G,B层）进行填充\n",
    "    \n",
    "    return:\n",
    "    @ x_padded: 经过填充之后的图像数据集\n",
    "    \"\"\"\n",
    "    \n",
    "    x_padded = np.pad(x, ((0,0),       # 第一维表示样本数，不填充\n",
    "                          (pad,pad),   # 图像高度，(x,y)可以视为上面填充ｘ个，下面填充ｙ个\n",
    "                          (pad,pad),   # 图像宽度，(x,y)可以视为左边填充ｘ个，右边填充ｙ个\n",
    "                          (0,0)),      # 通道数，不填充\n",
    "                      'constant', constant_values=0)  # 使用连续一样的值进行填充\n",
    "    return x_padded\n",
    "\n",
    "# 测试一下\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(1,3,3,2)\n",
    "x_padded = zero_pad(x, pad=2)\n",
    "fig, axeses = plt.subplots(1,2)\n",
    "axeses[0].set_title('x array')\n",
    "axeses[0].imshow(x[0,:,:,0])\n",
    "axeses[1].set_title('x_padded')\n",
    "axeses[1].imshow(x_padded[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 卷积神经网络的前向传播\n",
    "**单步卷积**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.628990515459734\n"
     ]
    }
   ],
   "source": [
    "def conv_single_step(slice_prev, w, b):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    @ slice_prev: 输入数据的一个片段，截取自前一层网络的输出，维数为（滤波器大小，滤波器大小，输入数据通道数）\n",
    "    @ w: 权重参数，包含在一个矩阵中，维数与 slice_prev 一致（也可以通道数为１，表示对slice_prev的所有通道执行相同卷积）\n",
    "    @ b: 偏置参数，维数为 (1,1,1)\n",
    "    \n",
    "    return:\n",
    "    @ z: 单步卷积的计算结果\n",
    "    \"\"\"\n",
    "    a = np.multiply(slice_prev, w) + b\n",
    "    z = np.sum(a)\n",
    "    return z\n",
    "x = np.random.randn(3,3,2)\n",
    "w = np.random.randn(3,3,1)\n",
    "b = 1\n",
    "print(conv_single_step(x, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**前向传播（卷积层）**  \n",
    "在前向传播的过程中，在每一个卷积层中，我们将使用多种卷积核对输入的数据进行卷积操作，使用每个卷积核进行卷积之后都会产生一个2D的矩阵，将这些2D的矩阵堆叠起来就变成了三维的矩阵，卷积核的个数对应该矩阵的通道数。  \n",
    "\n",
    "卷积输出的维度计算：\n",
    "\\begin{equation}\n",
    "n_{H_{out}}=\\lfloor \\frac {n_{H_{in}}-f+2\\times p}{stride}\\rfloor +1\\\\\n",
    "n_{W_{out}}=\\lfloor \\frac {n_{W_{in}}-f+2\\times p}{stride}\\rfloor +1\\\\\n",
    "n_c=卷积核数目\n",
    "\\end{equation}\n",
    "$n_{H}$为图像数据的高  \n",
    "$n_{W}$为图像数据的宽  \n",
    "$stride$为卷积步长  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向传播中卷积层的实现\n",
    "def conv_forward(x_input, w, b, **kwargs):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    @ x_input: 神经网络中上一层的输出矩阵，维度为（m,n_H_prev,n_W_prev,n_C_prev）\n",
    "               m -> 样本容量\n",
    "               n_H_prev -> 图片高度\n",
    "               n_W_prev -> 图片宽度\n",
    "               n_C_prev -> 通道数\n",
    "    @ w: 权重矩阵，维度为（n_C,f,f,n_C_prev）\n",
    "         f        -> 卷积核大小\n",
    "         n_C_prev -> 上一层输出的通道数\n",
    "         n_C      -> 该层卷积核的数目\n",
    "    @ b: 偏置矩阵，维度为（n_C, 1）,每个卷积核有一个偏置矩阵\n",
    "    @ kwargs: 可以传入参数 stride 和参数 pad，分别表示卷积的步长和边缘填充的数目\n",
    "    \n",
    "    return: \n",
    "    @ z_array: 卷积输出，维度为（m,n_H,n_W,n_C）\n",
    "               m: 样本数\n",
    "               n_H: 输出图像的高度\n",
    "               n_W: 输出图像的宽度\n",
    "               n_C: 卷积核的数目（输出图像的通道数）\n",
    "    @ cache: 缓存一些卷积层的反向传播计算需要的数据\n",
    "    \"\"\"\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = x_input.shape\n",
    "    (n_C, f, f, n_C_prev) = w.shape\n",
    "    \n",
    "    stride = 1\n",
    "    pad = 0\n",
    "    if 'stride' in kwargs:\n",
    "        stride = kwargs['stride']\n",
    "    if 'pad' in kwargs:\n",
    "        pad = kwargs['pad']\n",
    "\n",
    "    n_H = (n_H_prev - f + 2 * pad) // stride + 1\n",
    "    n_W = (n_W_prev - f + 2 * pad) // stride + 1\n",
    "    \n",
    "    y_output = np.zeros((m, n_H, n_W, n_C))   # 初始化卷积结果的存储矩阵\n",
    "    \n",
    "    x_input_padded = zero_pad(x_input, pad)  # 对输入的矩阵进行0填充\n",
    "    \n",
    "    for i in range(m):                  # 对每个样本\n",
    "        x = x_input_padded[i]\n",
    "        for h in range(n_H):            # 对每个样本的每一行\n",
    "            for w in range(n_W):        # 对每个样本的每一列\n",
    "                for c in range(n_C):    # 对每一个通道\n",
    "                    vert_start = stride * h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = stride * w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    slice_prev = x[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    y_output[i, h, w, c] = conv_single_step(slice_prev, w[c,:,:,:])\n",
    "                    # 如果有激活函数，在这一步将 y_output[i,h,w,c]　传入激活函数\n",
    "    \n",
    "    cache = (x_input, w, b, stride, pad)\n",
    "    return (y_output, cache)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**前向传播（池化层）**  \n",
    "池化层会减少输入的宽度和高度，这样，在减少计算量的同时也使得特征检测器对其在输入中的位置更加稳定。\n",
    "1. 最大值池化层：在输入矩阵中滑动一个大小为 f×f 的窗口，选取窗口中的最大值，然后作为输出的一部分\n",
    "2. 均值池化层：在输入矩阵中滑动一个大小为 f×f 的窗口，计算窗口中的平均值，作为输出的一部分  \n",
    "\n",
    "池化层输出维度计算：\n",
    "\\begin{equation}\n",
    "n_H=\\lfloor \\frac {n_{H_{prev}}-f}{stride} \\rfloor + 1\\\\\n",
    "n_W=\\lfloor \\frac {n_{W_{prev}}-f}{stride} \\rfloor + 1\\\\\n",
    "n_C = n_{C_{prev}}\\ ，通道数不变\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(x_input, mode = 'max', **kwargs):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    @ x_input: 神经网络中上一层的输出矩阵，维度为（m,n_H_prev,n_W_prev,n_C_prev）\n",
    "               m -> 样本容量\n",
    "               n_H_prev -> 图片高度\n",
    "               n_W_prev -> 图片宽度\n",
    "               n_C_prev -> 通道数\n",
    "    @ kwargs: 可以传入 f,stride 分别表示池化层的大小以及滑动窗口的步长\n",
    "    @ mode: 表示池化的类型｛'max','mean'｝\n",
    "    \n",
    "    return:\n",
    "    @ y_output: 池化层的输出，维度为（m, n_H, n_W, n_C）\n",
    "    @ cache: 反向传播中需要用到的值\n",
    "    \"\"\"\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = x_input.shape\n",
    "    f = 2\n",
    "    stride = 2\n",
    "    if 'f' in kwargs: f = kwargs['f']\n",
    "    if 'stride' in kwargs: stride = kwargs['stride']\n",
    "    \n",
    "    n_H = (n_H_prev - f) // stride + 1\n",
    "    n_W = (n_W_prev - f) // stride + 1\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    y_output = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    for i in range(m):\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    # 每一个样本的每一个通道分别进行操作\n",
    "                    slice_prev = x_input[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    if 'max' == mode.lower():\n",
    "                        y_output[i, j, w, c] = np.max(slice_prev)\n",
    "                    elif 'mean' == mode.lower():\n",
    "                        y_output[i, j, w, c] = np.mean(slice_prev)\n",
    "    cache = (x_input, mode, f, stride)\n",
    "    return (y_output, cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 卷积神经网络中的反向传播\n",
    "在现在的深度学习框架中，只需要实现前向传播，框架负责反向传播的计算。\n",
    "在全连接网络中，我们通过计算成本函数的梯度来进行参数的更新，类似的，在卷积神经网络中，我们也可以通过计算出成本的导数来更新参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 池化层的反向传播\n",
    "因为下采样层的存在，采样层（池化层）的一个像素对应卷积层输出的一块像素区域（采样区域）。所以为了在反向传播中能够计算卷积层的敏感度，就必须先对下采样（池化层）的敏感度（$dz^{[l+1]}$）进行上采样，使其与卷积层的map的尺寸对应。  \n",
    "\n",
    "\\begin{equation}\n",
    "\\delta^{[l]}=up(\\delta^{[l+1]})* A^{[l]'}(z)\n",
    "\\end{equation}\n",
    "$[l+1]$为池化层，$[l]$为卷积层,　$up(\\delta^{[l+1]})$ 为对 $[l+1]$ 层敏感度的上采样，上采样的公式为：\n",
    "\\begin{equation}\n",
    "mask * \\delta^{[l+1]}\n",
    "\\end{equation}\n",
    "**对于池化层的反向传播而言，有以下两种情况：**  \n",
    "**1. 均值池化层（池化层的维度为n\\*n）** \n",
    "\\begin{equation}\n",
    "mask = \\frac {1}{n\\times n}\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "1 & 1 & \\cdots & 1 \\\\\n",
    "1 & 1 & \\cdots & 1 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & 1 & \\cdots & 1\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}\n",
    "**2. 最大值池化层**\n",
    "\\begin{equation}\n",
    "mask = 只有最大值所在的对应位置为１，其余位置为０的n\\times n　矩阵\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.5 ],\n",
       "         [0.5 ],\n",
       "         [0.75],\n",
       "         [0.75]],\n",
       "\n",
       "        [[0.5 ],\n",
       "         [0.5 ],\n",
       "         [0.75],\n",
       "         [0.75]],\n",
       "\n",
       "        [[1.  ],\n",
       "         [1.  ],\n",
       "         [1.25],\n",
       "         [1.25]],\n",
       "\n",
       "        [[1.  ],\n",
       "         [1.  ],\n",
       "         [1.25],\n",
       "         [1.25]]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_mask(x, mode):\n",
    "    m_h, m_w = x.shape\n",
    "    if 'mean' == mode.lower():\n",
    "        mask = np.ones((m_h, m_w)) / (m_h * m_w)\n",
    "    elif 'max' == mode.lower():\n",
    "        mask = x == np.max(x)\n",
    "    return mask\n",
    "\n",
    "# 定义池化层的反向传播\n",
    "def back_propagation_pool(dz, cache):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    @ dz: 池化层的敏感度，与池化层输出的维度一致\n",
    "    @ cache: 在池化层前向传播过程中所存储的参数\n",
    "             x_input：池化层的输入矩阵，即卷积层的输出 A_prev\n",
    "             mode: 池化层的模式\n",
    "             f: 池化层大小\n",
    "             stride: 池化步长\n",
    "    \n",
    "    return:\n",
    "    @ dA_prev: 池化层反向传播的输出，与 x_input 的尺寸一致\n",
    "    \"\"\"\n",
    "    \n",
    "    x_input, mode, f, stride = cache\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = x_input.shape\n",
    "    m, n_H, n_W, n_C = dz.shape\n",
    "    dA_prev = np.zeros_like(x_input, dtype = 'float64')\n",
    "    \n",
    "    for i in range(m):\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    slice_prev = x_input[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    mask = get_mask(slice_prev, mode)\n",
    "                    dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\\\n",
    "                        += np.multiply(mask, dz[i, h, w, c])\n",
    "    return dA_prev\n",
    "\n",
    "# 测试一下\n",
    "dz = np.array([[[[2],[3]],[[4],[5]]]])\n",
    "x_input = np.array([[[[1], [2], [3], [2]],\n",
    "                     [[2], [1], [1], [2]],\n",
    "                     [[1], [2], [4], [3]],\n",
    "                     [[3], [4], [5], [1]]]])\n",
    "mode = 'mean'\n",
    "f = 2\n",
    "stride = 2\n",
    "cache = (x_input, mode, f, stride)\n",
    "back_propagation_pool(dz, cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积层的反向传播\n",
    "我们假定每个卷积层ｌ都会接一个下采样层ｌ＋１。对于bp网络来说，要求得层ｌ的每个神经元对应的权值的权值更新，就需要先求得层ｌ的每一个神经元对应的敏感度$\\delta$（即全连接网络中的$\\frac {d_{cost}}{d_z}$），而根据bp的反向传播公式 $dA^{[l-1]}=\\omega^{[l]T}\\frac {d_{lose}}{d_{z^{[l]}}}$，要求该层的敏感度，就必须先求下一层的敏感度，然后乘以权值矩阵。  \n",
    "对于卷积层而言，卷积层的下一层为池化层，池化层的反向传播敏感度经过上采样之后得到的敏感度 $\\delta^{[l]}$ 即为卷积层反向传播的输入。卷积层求解反向传播的公式与 DNN 中的公式类似，唯一的区别是 DNN 中的权重矩阵 $\\omega$ 对应于卷积层中的卷积核，并且在反向传播时需要对卷积核进行 $180^o$ 翻转(先上下翻转，后左右翻转)。  \n",
    "\n",
    "**关于卷积层反向传播的推演**  \n",
    "假设 $l-1$ 层的输出 $a^{[l-1]}$ 是一个 $3\\times 3$ 矩阵，第 $l$ 层的卷积核 $\\omega^{[l]}$ 是一个 $2 \\times 2$ 矩阵，采用１像素的步幅，则输出 $z^{[l]}$ 是一个 $2\\times 2$ 矩阵，我们简化 $b^l$ 都是０，则有：\n",
    "\\begin{equation}\n",
    "a^{[l-1]}*\\omega^{[l]}=z^{[l]}\\qquad (1)\n",
    "\\end{equation}\n",
    "列出 $a,\\omega,z$ 的矩阵表达式如下：\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "a_{11} & a_{12} & a_{13}\\\\\n",
    "a_{21} & a_{22} & a_{23}\\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{array}\n",
    "\\right]\n",
    "*\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "\\omega_{11} & \\omega_{12}\\\\\n",
    "\\omega_{21} & \\omega_{22}\n",
    "\\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "z_{11} & z_{12}\\\\\n",
    "z_{21} & z_{22}\n",
    "\\end{array}\n",
    "\\right]\\qquad (2)\n",
    "\\end{equation}\n",
    "\n",
    "利用卷积的定义，很容易得出：\n",
    "\\begin{equation}\n",
    "\\begin{array}{c}\n",
    "z_{11}=a_{11}\\omega_{11}+a_{12}\\omega_{12}+a_{21}\\omega_{21}+a_{22}\\omega_{22}\\\\\n",
    "z_{12}=a_{12}\\omega_{12}+a_{13}\\omega_{13}+a_{22}\\omega_{22}+a_{23}\\omega_{23}\\\\\n",
    "\\cdots\n",
    "\\end{array}   \\qquad (3)\n",
    "\\end{equation}\n",
    "\n",
    "接着模拟反向求导的过程(在反向传播过程中从当前层传入前一层的矩阵$\\nabla a^{[l-1]}$)：\n",
    "\\begin{equation}\n",
    "\\nabla{a^{[l-1]}}=\\frac {\\partial{J(\\omega,b)}}{\\partial{a^{[l-1]}}}=\\frac {\\partial{J(\\omega,b)}}{z^{[l]}} \\frac {\\partial{z^{[l]}}}{\\partial{a^{[l-1]}}}=\\delta^{[l]} \\frac {\\partial{z^{[l]}}}{\\partial{a^{[l-1]}}} \\qquad (4)\n",
    "\\end{equation}\n",
    "\n",
    "从上面式子可以看出，对于 $a^{[l-1]}$ 的梯度误差 $\\nabla a^{[l-1]}$，等于第 $l$ 层的敏感度($\\delta^{l}$)乘以 $\\frac {\\partial z^{[l]}}{\\partial a^{[l-1]}}$　，而 $\\frac {\\partial z^{[l]}}{\\partial a^{[l-1]}}$ 对应于上面的例子中相关联的 $\\omega$ 的值。假设我们的 $z$ 矩阵对应的反向传播误差是 $\\delta_{11},\\delta_{12},\\delta_{21},\\delta_{22}$ ，则利用上面式(4)中卷积计算的几个等式，可以分别写出 $\\nabla a^{[l-1]}$ 的９个标量的梯度。  \n",
    "比如对于 $a_{11}$ 的梯度，由于在４个等式中 $a_{11}$ 只和 $z_{11}$ 有乘积关系，有：  \n",
    "$$\\nabla a_{11} = \\delta_{11}\\omega_{11}$$\n",
    "对于 $a_{12}$ 的梯度，在４个等式中，只和 $z_{11},z_{12}$ 有乘积关系，从而有：\n",
    "$$\\nabla a_{12} = \\delta_{11}\\omega_{12} + \\delta_{12}\\omega_{11}$$\n",
    "对于其余几项，分别有：\n",
    "\\begin{equation}\n",
    "\\nabla a_{13}=\\delta_{12}\\omega_{12}\\\\\n",
    "\\nabla a_{21}=\\delta_{11}\\omega_{21}+\\delta_{21}\\omega_{11}\\\\\n",
    "\\nabla a_{22}=\\delta_{11}\\omega_{22}+\\delta_{12}\\omega_{21}+\\delta_{21}\\omega_{12}+\\delta_{22}\\omega_{11}\\\\\n",
    "\\nabla a_{23}=\\delta_{12}\\omega_{22}+\\delta_{22}\\omega_{12}\\\\\n",
    "\\nabla a_{31}=\\delta_{21}\\omega_{21}\\\\\n",
    "\\nabla a_{32}=\\delta_{21}\\omega_{22}+\\delta_{22}\\omega_{21}\\\\\n",
    "\\nabla a_{33}=\\delta_{22}\\omega_{22}\n",
    "\\end{equation}\n",
    "上述９个式子其实可以用一个矩阵卷积的形式表示，即：\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & \\delta_{11} & \\delta_{12} & 0 \\\\\n",
    "0 & \\delta_{21} & \\delta_{22} & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{array}\n",
    "\\right]\n",
    "*\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "\\omega_{22} & \\omega_{21} \\\\\n",
    "\\omega_{12} & \\omega_{11}\n",
    "\\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "a_{11} & a_{12} & a_{13}\\\\\n",
    "a_{21} & a_{22} & a_{23}\\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**推导卷积层w，b的梯度**  \n",
    "根据上述的推演，就已经可以计算出卷积层反向传播的输出 $\\delta{[l-1]}$ 作为上一层反向传播的输入了，但是卷积层相比与池化层而言，还多了两个参数：卷积核 w 以及偏差 b，类似与 DNN 中的权重矩阵 w 以及偏差 b，需要在反向传播的过程中通过梯度下降进行更新。  \n",
    "注意到卷积层 $z^{[l]}$ 和 $w^{[l]}$ 以及 $b^{[l]} 的关系，有：  \n",
    "\n",
    "$$z^{[l]}=w^{[l]}*a^{[l-1]}+b^{[l]}\\ , 此处的 * 表示卷积操作$$  \n",
    "因此，当计算 w 的梯度时，有以下操作：  \n",
    "\\begin{equation}\n",
    "\\frac {\\partial{cost}}{\\partial{w^{[l]}}}=a^{[l-1]}*\\delta^{[l]}, \\ 此处的 * 也表示卷积操作\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
