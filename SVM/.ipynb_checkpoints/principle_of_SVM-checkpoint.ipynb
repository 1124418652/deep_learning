{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM推导\n",
    "## 线性可分支持向量机\n",
    "**目标**  \n",
    "在特征空间中找到一个分离超平面，能将实例分到不同的类。分离超平面对应于方程 $\\omega x+b=0$，它由法向量 $\\omega$ 和截距 $b$ 决定，可以用($\\omega$，$b$)来表示。分离超平面将特征空间分为两部分，一部分是正类，一部分是负类。法线方向指向正类。  \n",
    "\n",
    "---\n",
    "**函数间隔**  \n",
    "1. 超平面关于样本点的函数间隔  \n",
    "对于给定的超平面 ($\\omega$，$b$) 和训练数据集，定义超平面关于样本点 ($x_i$，$y_i$) 的函数间隔为：\n",
    "$$\\hat {\\gamma_i}=y_i(\\omega x_i+b)$$\n",
    "2. 超平面关于训练数据集的函数间隔  \n",
    "定义超平面（$\\omega$，$b$）关于数据集 T 的函数间隔为超平面关于数据集中所有样本点的函数间隔的最小值，即：\n",
    "$$\\hat {\\gamma}= min_{i = 1,...,N}\\ {\\hat {(\\gamma_i)}}$$\n",
    "\n",
    "函数间隔可以表示分类预测的正确性及确性度，但是选择分离超平面时，只有函数间隔还不够，因为只要成比例地改变 $\\omega$ 和 b，例如将它们改为 2$\\omega$ 和 2b，超平面并没有改变，但是函数间隔却成为原来的 2 倍。   \n",
    "\n",
    "---\n",
    "**几何间隔**  \n",
    "点到直线或平面的距离：$\\hat {\\gamma_i} = \\frac {\\omega x +b}{\\|\\omega\\|}$  \n",
    "1. 超平面关于样本点的几何间隔  \n",
    "对于给定的超平面 ($\\omega$，$b$) 和训练数据集，定义超平面关于样本点 ($x_i$，$y_i$) 的几何间隔为：\n",
    "$${\\gamma_i}=y_i(\\frac {\\omega}{\\|\\omega\\|} x_i + \\frac {b}{\\|\\omega\\|})$$\n",
    "2. 超平面关于训练数据集的几何间隔  \n",
    "定义超平面（$\\omega$，$b$）关于数据集 T 的函数间隔为超平面关于数据集中所有样本点的函数间隔的最小值，即：\n",
    "$${\\gamma}= min_{i = 1,...,N}\\ {{(\\gamma_i)}}$$\n",
    "\n",
    "---\n",
    "**硬间隔最大化**  \n",
    "支持向量机学习的基本思想是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。对线性可分的训练数据集而言，线性可分分离超平面有无穷多个（等价于感知机），但是几何间隔最大的分离超平面是唯一的。  \n",
    "1. 求解最大间隔分离超平面  \n",
    "这个问题可以表示为下面的约束最优化问题：\n",
    "\\begin{equation}\n",
    "max_{\\omega,b}\\quad \\gamma\\\\\n",
    "s.t.\\quad y_i(\\frac {\\omega}{\\|\\omega\\|}\\cdot x_i+\\frac {b}{\\|\\omega\\|})\\ge \\gamma,\\qquad i=1,2,...,N\n",
    "\\end{equation}\n",
    "考虑几何间隔和函数间隔的关系式，这个问题可以转换为：\n",
    "\\begin{equation}\n",
    "max_{\\omega,b}\\quad \\frac {\\hat \\gamma}{\\|\\omega\\|}\\\\\n",
    "s.t.\\quad y_i(\\omega \\cdot x_i+b)\\ge \\hat \\gamma,\\qquad i=1,2,...,N\n",
    "\\end{equation}\n",
    "由于函数间隔 $\\hat \\gamma$ 的取值并不影响最优化问题的解，这样就可以取 $\\hat \\gamma=1$。上述的约束最优化问题就转换为了：\n",
    "\\begin{equation}\n",
    "min_{\\omega, b}\\quad \\frac 12 \\|\\omega\\|^2 \\\\\n",
    "s.t.\\quad y_i(\\omega \\cdot x_i+b)-1 \\ge 0\n",
    "\\end{equation}\n",
    "\n",
    "2. 支持向量和间隔边界  \n",
    "在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量，支持向量是使约束条件式等号成立的点，即 $y_i(\\omega x_i+b)-1=0$，对于 $y_i = +1$ 的正例点，支持向量在超平面：$H1:\\omega \\cdot x+b=1$ 上；对于 $y_i = -1$ 的负例点，支持向量在 $H2:\\omega \\cdot x+b = -1$ 上。  \n",
    "超平面 H1 与 H2 之间的距离称为间隔：$\\frac {2}{\\|\\omega\\|}$  \n",
    "在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用，所以这种分类模型称为支持向量机。  \n",
    "\n",
    "\n",
    "3. 学习的对偶算法  \n",
    "为了求解线性可分支持向量机的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶型，通过求解对偶问题得到原始问题的最优解。这样做的优点：一是对偶问题往往更容易求解；二是自然引入核函数，进而推广到非线性分类问题。  \n",
    "**1) 原始问题**  \n",
    "首先构建拉格朗日函数。为此，对每个不等式约束引进拉格朗如乘子 $\\alpha_i\\ge0,i=1,2,...,N$，定义拉格朗日函数：  \n",
    "构建拉格朗日的步骤：  \n",
    "a. 原始问题为最小值问题，即 $min_x(f(x))$  \n",
    "b. 约束条件写成 $\\le 0$ 的形式  \n",
    "c. 约束条件乘以拉格朗日乘子\n",
    "\\begin{equation}\n",
    "L(\\omega, b, \\alpha)=\\frac 12 \\|\\omega\\|^2+\\sum_{i=1}^{N}\\alpha_i[1-y_i(\\omega x_i+b)]\\\\=\n",
    "\\frac 12 \\|\\omega\\|^2 - \\sum_{i=1}^{N}\\alpha_i y_i (\\omega x_i+b)+\\sum_{i=1}^{N}\\alpha_i\n",
    "\\end{equation}\n",
    "在上述式子中，由于 $\\alpha_i [1-y_i(\\omega x_i+b)]\\le0$，所以 $max_{\\alpha}\\{L(\\omega, b, \\alpha)\\}$ 等同于 $\\frac 12 \\|\\omega\\|^2$，而原始问题 $min\\{\\frac 12 \\|\\omega\\|^2\\}$ 就可以转换为 $min_{\\omega, b}\\ max_{\\alpha}\\ L(\\omega, b, \\alpha)$  \n",
    "**2) 对偶问题**  \n",
    "在上述过程中，已经将原始问题转换为了极小极大问题 $min_{\\omega, b}\\ max_{\\alpha}\\ L(\\omega, b, \\alpha)$，但是该问题并不好求解，如果可以将其转换为它的对偶问题 $max_{\\alpha}min_{\\omega, b}\\ L(\\omega, b, \\alpha)$，就容易求解多了，定义：\n",
    "\\begin{equation}\n",
    "L(\\omega, b, \\alpha)=\\frac 12 \\|\\omega\\|^2 - \\sum_{i=1}^{N}\\alpha_i y_i (\\omega x_i+b)+\\sum_{i=1}^{N}\\alpha_i \\qquad(1)\\\\\n",
    "min_{\\omega, b}\\ L(\\omega, b, \\alpha)= min_{\\omega, b}\\ \\frac {1}{\\|\\omega\\|}+min_{\\omega, b}\\ \\alpha g(\\omega, b)，其中 g(\\omega, b)=\\sum_{i=1}^{N}1-y_i (\\omega x_i+b)\\\\\n",
    "\\end{equation}\n",
    "其中：\n",
    "\\begin{equation}\n",
    "min_{\\omega, b}\\ \\alpha g(\\omega, b)=\n",
    "\\begin{cases}\n",
    "0,\\quad \\alpha = 0\\ or\\ g(\\omega, b)=0\\\\\n",
    "-\\infty,\\quad others\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "并且，当 $\\omega,b$ 为 $L(\\omega, b, \\alpha)$ 的极小值时，存在 $\\frac {\\partial L}{\\partial w}=0$，以上两项加上$\\alpha\\ge0$即为KKT条件  \n",
    "\n",
    "\n",
    "4. 对偶问题的求解  \n",
    "**1) 先求解 $min_{\\omega, b}\\ L(\\omega, b, \\alpha)$**  \n",
    "\\begin{equation}\n",
    "\\frac {\\partial L}{\\partial \\omega}=\\omega-\\sum_{i=1}^{N}\\alpha_i y_i x_i=0 \\qquad(2)\\\\\n",
    "\\frac {\\partial L}{\\partial b}=-\\sum_{i=1}^{N}\\alpha_i y_i=0 \\qquad(3)\n",
    "\\end{equation}\n",
    "   将(2),(3)带入(1)即可得：\n",
    "\\begin{equation}\n",
    "min_{\\omega, b}\\ L(\\omega, b, \\alpha)=-\\frac 12\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j (x_i\\centerdot x_j)+\\sum_{i=1}^{N}\\alpha_{i}\n",
    "\\end{equation}\n",
    "**2) 求解 $max_{\\alpha}\\ min_{\\omega, b}L$**  \n",
    "\\begin{equation}\n",
    "max_{\\alpha}\\quad -\\frac 12\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j (x_i\\centerdot x_j)+\\sum_{i=1}^{N}\\alpha_{i}\\quad(4)\\\\\n",
    "s.t. \\quad\\sum_{i = 1}^{N}\\alpha_i y_i = 0\n",
    "\\end{equation}\n",
    "将(4)从求极大转换为求极小的问题(为了可以用拉格朗日法求解)：\n",
    "\\begin{equation}\n",
    "min_{\\alpha}\\quad \\frac 12\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j (x_i\\centerdot x_j)-\\sum_{i=1}^{N}\\alpha_{i}\\quad(5)\n",
    "\\end{equation}\n",
    "从而最初的问题就转换成了以下问题的求解：\n",
    "\\begin{equation}\n",
    "min_{\\alpha}\\quad \\frac 12\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j (x_i\\centerdot x_j)-\\sum_{i=1}^{N}\\alpha_{i}\\\\\n",
    "s.t. \\quad\\sum_{i = 1}^{N}\\alpha_i y_i = 0\\\\\n",
    "\\alpha_i\\ge0\n",
    "\\end{equation}\n",
    "并且如果得到了上述问题的最优解 $\\alpha^*=(\\alpha_1^*,\\alpha_2^*,...,\\alpha_N^*)$，则可按下式求得 $\\omega,b$ 的最优解：\n",
    "\\begin{equation}\n",
    "\\omega^*=\\sum_{i-1}^{N}\\alpha_i^*y_i x_i\\\\\n",
    "b^*=y_j-\\sum_{j=1}^{N}\\alpha^*_i y_i(x_i\\centerdot x_j)\n",
    "\\end{equation}\n",
    "由之前的KKT条件，已知必须存在 $\\alpha_i [1-y_i(\\omega x_i)+b]=0$，上述对偶问题才成立，而支持向量机中 $\\omega,b$ 的更新只与支持向量，即 $[1-y_i(\\omega x_i)+b]=0$ 有关，此时有 $\\alpha_i>0$  \n",
    "\n",
    "---\n",
    "**软间隔最大化**  \n",
    "线性可分问题的支持向量学习方法，对线性不可分训练数据是不适用的，线性不可分意味着某些样本点 $(x_j,y_j)$ 不能满足函数间隔大于１的约束条件。为了解决这个问题，可以对每个样本点 $(x_i,y_i)$ 引进一个松弛变量 $\\xi\\ge0$，使函数间隔加上松弛变量大于等于１，这样约束条件变为：\n",
    "\\begin{equation}\n",
    "y_i(\\omega x_i+b)\\ge 1-\\xi_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
