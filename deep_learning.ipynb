{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数  \n",
    "___\n",
    "### 神经网络中的激活函数需要满足的条件：\n",
    "1. 非线性：即导数不是常数。这个条件可以保证多层网络不退化成单层线性网络。 \n",
    "\n",
    "\n",
    "2. 几乎处处可微：可微保证了在优化中梯度的课计算性。\n",
    "\n",
    "\n",
    "3. 计算简单：激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。\n",
    "\n",
    "\n",
    "4. 非饱和性：饱和性是指在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新。\n",
    "    * sigmod函数：它的导数在x为比较大的正值和比较小的负值时都会接近于0。\n",
    "    * 阶跃函数：几乎所有位置梯度都为0，所以无法作为激活函数。\n",
    "    * ReLU函数：在x>0时梯度恒为1，不会饱和，但在x<0时，梯度恒为0，出现饱和。\n",
    "\n",
    "\n",
    "5. 单调性：即导数符号不变\n",
    "\n",
    "\n",
    "6. 输出范围有限：有限的输出范围使得网络对于一些较大的输入也会比较稳定。\n",
    "\n",
    "\n",
    "7. 接近恒等变换：即约等于x。这样的好处是使得输出的幅值不会随着深度的增加而发生显著的增加，从而使网络更为稳定。这个与非线性是有点矛盾的，因此激活函数基本只是部分满足这个条件，比如TanH只在原点附近有线性区（在原点为0且在原点的导数为1），而ReLU只在x>0时为线性。这个性质也让初始化参数范围的推导更为简单。\n",
    "\n",
    "\n",
    "8. 参数少\n",
    "\n",
    "\n",
    "9. 归一化：这个是最近才出来的概念，对应的激活函数是SELU，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如Batch Normalization。\n",
    "\n",
    "\n",
    "10. zero-centered：Sigmoid函数的输出值恒大于0，这会导致模型训练的收敛速度变慢。\n",
    "\n",
    "___\n",
    "### 常用的激活函数：\n",
    "**1. sigmod函数**  \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "g(z)=\\frac{1}{1+e^{-z}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "g'(z)=g(z)[1-g(z)]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**2. tanh函数**\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "g(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "g'(z)=1-[g(z)]^2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**3. ReLU函数** \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "g(z)=max(0,z)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "g'(z)=\n",
    "\\begin{cases}\n",
    "0,\\quad if \\quad z<0\\\\\n",
    "1,\\quad if \\quad z\\ge0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**4. Leaky ReLU函数**\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "g(z)=max(0.01z,z)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "g'(z)=\n",
    "\\begin{cases}\n",
    "0.01,\\quad if \\quad z<0\\\\\n",
    "1,\\quad if \\quad z\\geq0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络的训练\n",
    "### 前向传播：\n",
    "\\begin{equation}\n",
    "Z^{[1]}=w^{[1]}x+b^{[1]}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "A^{[1]}=g^{[1]}(Z^{[1]})\\\\\n",
    "Z^{[2]}=w^{[2]}A^{[1]}+b^{[2]}\\\\\n",
    "A^{[2]}=g^{[2]}(Z^{[2]})\n",
    "\\end{equation}\n",
    "___\n",
    "### 反向传播：\n",
    "1. Lose Function:$$L=-\\frac 1m \\sum _{i=0}^{m-1}[(1-y_i)log(1-\\hat y_i)+y_ilog(\\hat y_i)]$$\n",
    "2. 计算$dw^{[2]},db^{[2]}:$\n",
    "\\begin{equation}\n",
    "dZ^{[2]}=\\frac 1m (A^{[2]}-Y),\\qquad Y=[y^{(1)},y^{(2)},...,y^{(m)}]\\\\\n",
    "dw^{[2]}=\\frac 1m dZ^{[2]}A^{[1]T}\\\\\n",
    "db^{[2]}=\\frac 1m dZ^{[2]}\n",
    "\\end{equation}\n",
    "3. 计算$dw^{[1]},db^{[1]}:$\n",
    "\\begin{equation}\n",
    "dZ^{[1]}=W^{[2]T}dZ^{[2]}*A^{[1]'}(Z^{[1]})\\\\\n",
    "dw^{[1]}=dZ^{[1]}x^T\\\\\n",
    "db^{[1]}=dZ^{[1]}.sum(axix=1)\n",
    "\\end{equation}\n",
    "___\n",
    "### 维数计算：\n",
    "输入 x：（data_dim, data_num）  \n",
    "隐藏层 w1_array：（n1, data_dim）  \n",
    "b1：（n1, 1）  \n",
    "z1, a1：（n1, data_num）\n",
    "w2_array：（n2, n1）  \n",
    "b2：（n2, 1）  \n",
    "z2, a2：（n2, data_num）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "import numpy as np\n",
    "from math import *\n",
    "\n",
    "class BPANN(object):\n",
    "    def __init__(self, layer, *layer_nodes):\n",
    "        self.layer = layer\n",
    "        \n",
    "        if layer != len(layer_nodes):\n",
    "            print(\"Can't initial the neuron network!\")\n",
    "            exit(1)\n",
    "        \n",
    "        self.layer_nodes = layer_nodes\n",
    "        \n",
    "    def load_data(self, data_set, labels):\n",
    "        self.data_set = np.mat(data_set).T\n",
    "        self.labels = np.mat(labels)\n",
    "        self.data_size = len(data_set)\n",
    "        self.data_dim = len(data_set[0])\n",
    "        self.w1_array = np.mat(np.random.randn(self.layer_nodes[0], self.data_dim) * 0.01)\n",
    "        self.w2_array = np.mat(np.random.randn(self.layer_nodes[1], self.layer_nodes[0]) * 0.01)\n",
    "        self.b1 = np.mat(np.zeros(self.layer_nodes[0])).T\n",
    "        self.b2 = np.zeros(self.layer_nodes[1])\n",
    "        \n",
    "    def __for_prop(self, x):\n",
    "        # forward propagation\n",
    "        z1 = np.dot(self.w1_array, np.mat(x).T) + self.b1\n",
    "        # print(self.w1_array, self.b1, z1)\n",
    "        a1 = 1 / (1 + np.exp(-z1))\n",
    "        z2 = np.dot(self.w2_array, a1) + self.b2\n",
    "        a2 = 1 / (1 + np.exp(-z2))\n",
    "        return a2\n",
    "    \n",
    "    def training(self, iterate = 100):\n",
    "        for i in range(iterate):\n",
    "            z1 = self.w1_array * self.data_set + self.b1\n",
    "            a1 = 1 / (1 + np.exp(-z1))\n",
    "            z2 = self.w2_array * a1 + self.b2\n",
    "            a2 = 1 / (1 + np.exp(-z2))\n",
    "            \n",
    "            dz2 = a2 - self.labels\n",
    "            db2 = 1 / self.data_size * dz2.sum()\n",
    "            dw2 = 1 / self.data_size * dz2 * a1.T\n",
    "            dz1 = 1 / self.data_size * np.multiply(self.w2_array.T * dz2, np.multiply(a1, (1 - a1)))\n",
    "            dw1 = dz1 * self.data_set.T\n",
    "            db1 = dz1.sum(axis = 1)\n",
    "            \n",
    "            self.w2_array -= dw2\n",
    "            self.b2 -= db2\n",
    "            self.w1_array -= dw1\n",
    "            self.b1 -= db1\n",
    "        \n",
    "        #print(self.w1_array, self.b1, self.w2_array, self.b2)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        if self.__for_prop(x) >= 0.5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "def main():\n",
    "    data_set = [[3, 2],\n",
    "               [1, 1],\n",
    "               [2, 1]]\n",
    "    labels = [1, 0, 1]\n",
    "    bpan = BPANN(2, 4, 1)\n",
    "    bpan.load_data(data_set, labels)\n",
    "    bpan.training()\n",
    "    print(bpan.predict([1,1]))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "a = np.mat([[1,2,3],\n",
    "           [2,3,4]])\n",
    "c = np.mat([[1],[1]])\n",
    "print()\n",
    "# print(np.exp(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
